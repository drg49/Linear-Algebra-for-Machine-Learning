{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae1cc9e",
   "metadata": {},
   "source": [
    "# Logistic Regression from Scratch\n",
    "## Binary Classification on the Titanic Dataset\n",
    "\n",
    "This notebook implements logistic regression using pure NumPy to predict passenger survival on the Titanic. We'll build the algorithm from first principles, including the sigmoid function, loss function, and gradient descent optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f487981e",
   "metadata": {},
   "source": [
    "## Section 1: Load and Explore the Titanic Dataset\n",
    "\n",
    "We'll start by importing NumPy and loading a clean subset of the Titanic dataset. The dataset contains:\n",
    "- **Pclass**: Passenger class (1, 2, or 3)\n",
    "- **Sex**: 0 = female, 1 = male\n",
    "- **Age**: Passenger age\n",
    "- **Fare**: Ticket fare paid\n",
    "- **Survived**: Target variable (0 = died, 1 = survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e5e678a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic Dataset:\n",
      "    Pclass  Sex   Age   Fare  Survived\n",
      "0        3    1  22.0   7.25         0\n",
      "1        1    0  38.0  71.28         1\n",
      "2        3    0  26.0   7.92         1\n",
      "3        1    0  35.0  53.10         1\n",
      "4        3    1  35.0   8.05         0\n",
      "5        3    1  30.0   8.46         0\n",
      "6        2    1  34.0  21.00         0\n",
      "7        2    0  28.0  26.00         1\n",
      "8        3    0   2.0  21.07         1\n",
      "9        3    1  40.0  15.50         0\n",
      "10       1    1  58.0  26.55         1\n",
      "11       3    0  15.0   7.22         1\n",
      "12       3    1  28.0   7.90         0\n",
      "13       1    1  42.0  26.55         0\n",
      "14       3    0  19.0   3.17         1\n",
      "\n",
      "Dataset shape: (15, 5)\n",
      "\n",
      "Data types:\n",
      "Pclass        int32\n",
      "Sex           int32\n",
      "Age         float64\n",
      "Fare        float64\n",
      "Survived      int32\n",
      "dtype: object\n",
      "\n",
      "Survival distribution:\n",
      "Survived\n",
      "1    8\n",
      "0    7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "X (features) shape: (15, 4)\n",
      "y (labels) shape: (15, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# REAL WORLD TITANIC DATASET (SMALL CLEAN SUBSET)\n",
    "# Columns: Pclass, Sex (0=female,1=male), Age, Fare, Survived (0=died, 1=survived)\n",
    "\n",
    "data = np.array([\n",
    "    [3, 1, 22, 7.25, 0],\n",
    "    [1, 0, 38, 71.28, 1],\n",
    "    [3, 0, 26, 7.92, 1],\n",
    "    [1, 0, 35, 53.10, 1],\n",
    "    [3, 1, 35, 8.05, 0],\n",
    "    [3, 1, 30, 8.46, 0],\n",
    "    [2, 1, 34, 21.00, 0],\n",
    "    [2, 0, 28, 26.00, 1],\n",
    "    [3, 0, 2, 21.07, 1],\n",
    "    [3, 1, 40, 15.50, 0],\n",
    "    [1, 1, 58, 26.55, 1],\n",
    "    [3, 0, 15, 7.22, 1],\n",
    "    [3, 1, 28, 7.90, 0],\n",
    "    [1, 1, 42, 26.55, 0],\n",
    "    [3, 0, 19, 3.17, 1],\n",
    "])\n",
    "\n",
    "# Create a DataFrame with descriptive column names\n",
    "df = pd.DataFrame(data, columns=['Pclass', 'Sex', 'Age', 'Fare', 'Survived'])\n",
    "df['Pclass'] = df['Pclass'].astype(int)\n",
    "df['Sex'] = df['Sex'].astype(int)\n",
    "df['Survived'] = df['Survived'].astype(int)\n",
    "\n",
    "print(\"Titanic Dataset:\")\n",
    "print(df.to_string(index=True))\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "print(f\"\\nSurvival distribution:\\n{df['Survived'].value_counts()}\")\n",
    "\n",
    "# Split into X (features) and y (labels) using numeric values\n",
    "X = data[:, :-1].astype(float)                      # features: shape (15, 4)\n",
    "y = data[:, -1].reshape(-1, 1).astype(float)        # labels: shape (15, 1)\n",
    "\n",
    "print(f\"\\nX (features) shape: {X.shape}\")\n",
    "print(f\"y (labels) shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cecbbef",
   "metadata": {},
   "source": [
    "## Section 2: Standardize Features\n",
    "\n",
    "Feature standardization is critical for gradient descent convergence. Features with larger scales (like Fare) can dominate the learning process. We standardize by:\n",
    "1. **Centering**: Subtract the mean of each feature (X - X_mean)\n",
    "2. **Scaling**: Divide by the standard deviation (/ X_std)\n",
    "\n",
    "This ensures all features have mean ≈ 0 and standard deviation ≈ 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf2b1045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature means:\n",
      "[ 2.33333333  0.53333333 30.13333333 20.73466667]\n",
      "\n",
      "Feature standard deviations:\n",
      "[ 0.86922699  0.49888765 12.58499989 18.32812515]\n",
      "\n",
      "Standardized X (first 5 samples):\n",
      "[[ 0.76696499  0.93541435 -0.64627202 -0.73573628]\n",
      " [-1.53392998 -1.06904497  0.62508278  2.75780163]\n",
      " [ 0.76696499 -1.06904497 -0.32843332 -0.69918044]\n",
      " [-1.53392998 -1.06904497  0.38670375  1.76588348]\n",
      " [ 0.76696499  0.93541435  0.38670375 -0.69208752]]\n",
      "\n",
      "Mean of standardized X (should be ~0):\n",
      "[-1.48029737e-16  4.44089210e-17  3.70074342e-17 -8.88178420e-17]\n",
      "\n",
      "Std of standardized X (should be ~1):\n",
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Standardize features\n",
    "X_mean = X.mean(axis=0)\n",
    "X_std  = X.std(axis=0)\n",
    "\n",
    "print(\"Feature means:\")\n",
    "print(X_mean)\n",
    "print(\"\\nFeature standard deviations:\")\n",
    "print(X_std)\n",
    "\n",
    "# Standardize: (X - mean) / std\n",
    "X = (X - X_mean) / X_std\n",
    "\n",
    "print(\"\\nStandardized X (first 5 samples):\")\n",
    "print(X[:5])\n",
    "print(\"\\nMean of standardized X (should be ~0):\")\n",
    "print(X.mean(axis=0))\n",
    "print(\"\\nStd of standardized X (should be ~1):\")\n",
    "print(X.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd5712a",
   "metadata": {},
   "source": [
    "## Section 3: Implement the Sigmoid Function\n",
    "\n",
    "The sigmoid function maps any real number to a probability between 0 and 1:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "This function is the key to converting linear predictions into probabilities for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03417905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    Maps input to probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Test the sigmoid function\n",
    "z_test = np.array([-5, -1, 0, 1, 5])\n",
    "sigmoid_out = sigmoid(z_test)\n",
    "print(\"Sigmoid function test:\")\n",
    "for z, s in zip(z_test, sigmoid_out):\n",
    "    print(f\"sigmoid({z:2}) = {s:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a136b4",
   "metadata": {},
   "source": [
    "## Section 4: Implement Binary Cross Entropy Loss\n",
    "\n",
    "Binary cross entropy measures the difference between predicted probabilities and actual labels:\n",
    "\n",
    "$$L = -\\frac{1}{m}\\sum_{i=1}^{m}[y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]$$\n",
    "\n",
    "We add a small epsilon value to prevent log(0) numerical errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7669c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_true, y_prediction):\n",
    "    \"\"\"\n",
    "    Binary cross entropy loss function.\n",
    "    Measures the difference between predicted probabilities and actual labels.\n",
    "    \"\"\"\n",
    "    eps = 1e-9  # Small epsilon to avoid log(0)\n",
    "    return -np.mean(\n",
    "        y_true * np.log(y_prediction + eps) +\n",
    "        (1 - y_true) * np.log(1 - y_prediction + eps)\n",
    "    )\n",
    "\n",
    "# Test the loss function\n",
    "y_true_test = np.array([[1], [0], [1]])\n",
    "y_pred_test = np.array([[0.9], [0.1], [0.7]])\n",
    "loss = loss_fn(y_true_test, y_pred_test)\n",
    "print(f\"Binary Cross Entropy Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc27a01",
   "metadata": {},
   "source": [
    "## Section 5: Utility Function and Initialize Model Parameters\n",
    "\n",
    "We'll define a helper function to display predictions and initialize our model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa41d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_survival_probs(probs):\n",
    "    \"\"\"\n",
    "    Print predicted survival probabilities for each passenger.\n",
    "    Uses 0.5 as the classification threshold.\n",
    "    \"\"\"\n",
    "    probs = probs.ravel()\n",
    "    for index, p in enumerate(probs):\n",
    "        status = \"Survived\" if p >= 0.5 else \"Died\"\n",
    "        print(f\"({index:02d}) {p:.3f} -> {status}\")\n",
    "\n",
    "# Initialize model parameters\n",
    "n_features = X.shape[1]  # 4 features: Pclass, Sex, Age, Fare\n",
    "w = np.zeros((n_features, 1))  # weights: shape (4, 1)\n",
    "b = 0.0                        # bias: scalar\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "epochs = 2000\n",
    "\n",
    "# Store loss history for visualization\n",
    "loss_history = []\n",
    "\n",
    "print(\"Initial model parameters:\")\n",
    "print(f\"Weights shape: {w.shape}\")\n",
    "print(f\"Weights: {w.ravel()}\")\n",
    "print(f\"Bias: {b}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Epochs: {epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2778ad7a",
   "metadata": {},
   "source": [
    "## Section 6: Training Loop with Gradient Descent\n",
    "\n",
    "The training loop performs:\n",
    "1. **Forward pass**: Compute z = X @ w + b, then y_pred = sigmoid(z)\n",
    "2. **Loss calculation**: Compute binary cross entropy loss\n",
    "3. **Backward pass**: Compute gradients (dw, db)\n",
    "4. **Parameter update**: Update w and b using gradient descent\n",
    "\n",
    "Gradients:\n",
    "- $\\frac{\\partial L}{\\partial z} = \\hat{y} - y$\n",
    "- $\\frac{\\partial L}{\\partial w} = \\frac{1}{m}X^T(\\hat{y} - y)$\n",
    "- $\\frac{\\partial L}{\\partial b} = \\frac{1}{m}\\sum(\\hat{y} - y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ebf10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass: predict probabilities\n",
    "    z = X @ w + b\n",
    "    y_prediction = sigmoid(z)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_fn(y, y_prediction)\n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    # Backward pass: compute gradients\n",
    "    dz = y_prediction - y\n",
    "    dw = (X.T @ dz) / len(X)\n",
    "    db = np.mean(dz)\n",
    "    \n",
    "    # Update parameters\n",
    "    w -= learning_rate * dw\n",
    "    b -= learning_rate * db\n",
    "    \n",
    "    # Print progress every 200 epochs\n",
    "    if (epoch % 200) == 0:\n",
    "        print(f\"\\n--- Epoch {epoch} ---\")\n",
    "        print(f\"Weights: {w.ravel()}\")\n",
    "        print(f\"Bias: {b:.6f}\")\n",
    "        print(f\"Linear combination (z = X @ w + b):\")\n",
    "        print(z.ravel())\n",
    "        print(f\"\\nY Predictions:\")\n",
    "        print_survival_probs(y_prediction)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        pred_labels = (y_prediction >= 0.5).astype(int)\n",
    "        acc = (pred_labels == y).mean()\n",
    "        print(f\"\\nLoss={loss:.4f} Accuracy={acc:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabae6c7",
   "metadata": {},
   "source": [
    "## Section 7: Evaluate Model Performance\n",
    "\n",
    "Now let's evaluate the trained model's performance on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f11a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "final_z = X @ w + b\n",
    "final_probs = sigmoid(final_z)\n",
    "final_preds = (final_probs >= 0.5).astype(int)\n",
    "final_acc = (final_preds == y).mean()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nFinal Accuracy: {final_acc:.4f} ({final_acc*100:.2f}%)\")\n",
    "print(f\"\\nFinal Weights:\")\n",
    "print(f\"  w[0] (Pclass):  {w[0, 0]:.6f}\")\n",
    "print(f\"  w[1] (Sex):     {w[1, 0]:.6f}\")\n",
    "print(f\"  w[2] (Age):     {w[2, 0]:.6f}\")\n",
    "print(f\"  w[3] (Fare):    {w[3, 0]:.6f}\")\n",
    "print(f\"\\nFinal Bias: {b:.6f}\")\n",
    "\n",
    "print(f\"\\n\\nFinal Predictions vs Actual:\")\n",
    "print(f\"{'Index':<8} {'Predicted':<12} {'Actual':<8} {'Prob':<8}\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(len(y)):\n",
    "    pred = final_preds[i, 0]\n",
    "    actual = y[i, 0]\n",
    "    prob = final_probs[i, 0]\n",
    "    match = \"✓\" if pred == actual else \"✗\"\n",
    "    print(f\"{i:<8} {int(pred):<12} {int(actual):<8} {prob:.4f} {match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242b3d91",
   "metadata": {},
   "source": [
    "## Section 8: Visualize Predictions and Learning Progress\n",
    "\n",
    "Let's create visualizations to understand our model's performance and convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a05c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Create a figure with multiple subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Loss over epochs\n",
    "ax = axes[0, 0]\n",
    "ax.plot(loss_history, linewidth=2, color='blue')\n",
    "ax.set_xlabel('Epoch', fontsize=11)\n",
    "ax.set_ylabel('Loss (Binary Cross Entropy)', fontsize=11)\n",
    "ax.set_title('Training Loss Over Epochs', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Survival probabilities\n",
    "ax = axes[0, 1]\n",
    "colors = ['red' if p >= 0.5 else 'green' for p in final_probs.ravel()]\n",
    "ax.bar(range(len(final_probs)), final_probs.ravel(), color=colors, alpha=0.7)\n",
    "ax.axhline(y=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "ax.set_xlabel('Passenger Index', fontsize=11)\n",
    "ax.set_ylabel('Survival Probability', fontsize=11)\n",
    "ax.set_title('Predicted Survival Probabilities', fontsize=12, fontweight='bold')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Confusion Matrix\n",
    "ax = axes[1, 0]\n",
    "cm = confusion_matrix(y.ravel(), final_preds.ravel())\n",
    "im = ax.imshow(cm, cmap='Blues', aspect='auto')\n",
    "ax.set_xlabel('Predicted', fontsize=11)\n",
    "ax.set_ylabel('Actual', fontsize=11)\n",
    "ax.set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels(['Died', 'Survived'])\n",
    "ax.set_yticklabels(['Died', 'Survived'])\n",
    "# Add text annotations\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\", fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# 4. Prediction accuracy by class\n",
    "ax = axes[1, 1]\n",
    "class_names = ['Died (0)', 'Survived (1)']\n",
    "accuracies = []\n",
    "for class_label in [0, 1]:\n",
    "    mask = y.ravel() == class_label\n",
    "    if np.sum(mask) > 0:\n",
    "        class_acc = (final_preds.ravel()[mask] == y.ravel()[mask]).mean()\n",
    "        accuracies.append(class_acc)\n",
    "    else:\n",
    "        accuracies.append(0)\n",
    "\n",
    "bars = ax.bar(class_names, accuracies, color=['coral', 'lightblue'], alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax.set_ylabel('Accuracy', fontsize=11)\n",
    "ax.set_title('Class-wise Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_ylim([0, 1.1])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.2%}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y.ravel(), final_preds.ravel(), target_names=['Died', 'Survived']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b563185a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've successfully implemented logistic regression from scratch using NumPy! Here's what we learned:\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Standardization**: Normalizes features for better gradient descent convergence\n",
    "2. **Sigmoid Function**: Maps linear combinations to probabilities [0, 1]\n",
    "3. **Binary Cross Entropy**: Measures prediction error\n",
    "4. **Gradient Descent**: Iteratively updates weights and bias to minimize loss\n",
    "5. **Backpropagation**: Computes gradients through the chain rule\n",
    "\n",
    "### Model Architecture:\n",
    "- **Linear layer**: z = X·w + b\n",
    "- **Activation**: sigmoid(z) = 1 / (1 + e^-z)\n",
    "- **Output**: Probability of survival\n",
    "\n",
    "### Performance:\n",
    "The model learns to classify survival outcomes based on passenger features (Pclass, Sex, Age, Fare). The visualizations show:\n",
    "- **Loss curve**: Demonstrates convergence over epochs\n",
    "- **Probabilities**: Distribution of predicted survival chances\n",
    "- **Confusion matrix**: Count of correct/incorrect predictions\n",
    "- **Class accuracy**: Per-class performance metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
